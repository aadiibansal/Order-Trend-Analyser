{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0b3ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import smtplib\n",
    "import logging\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# Utils modules\n",
    "from utils import *\n",
    "\n",
    "# Xplotter\n",
    "from xplotter.insights import *\n",
    "from xplotter.formatter import format_spines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3005f9f",
   "metadata": {},
   "source": [
    "FETCHING DATA FROM SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1f561dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORDERID</th>\n",
       "      <th>CHANNELID</th>\n",
       "      <th>ORDERTYPE</th>\n",
       "      <th>ORDERSTATUS</th>\n",
       "      <th>CREATIONDATE</th>\n",
       "      <th>LASTUPDATEDDATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTLWT07HI04104</td>\n",
       "      <td>WEB</td>\n",
       "      <td>PURCHASEEQUIPMENT</td>\n",
       "      <td>Completed</td>\n",
       "      <td>03-APR-22 02.44.06 AM</td>\n",
       "      <td>03-APR-22 02.44.28 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CTLWT07ND04290</td>\n",
       "      <td>WEB</td>\n",
       "      <td>CHANGEMSISDN</td>\n",
       "      <td>Completed</td>\n",
       "      <td>03-APR-22 02.45.13 AM</td>\n",
       "      <td>03-APR-22 02.45.38 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CTLWT04XX99611</td>\n",
       "      <td>WEB</td>\n",
       "      <td>CHANGEMSISDN</td>\n",
       "      <td>Completed</td>\n",
       "      <td>03-APR-22 01.46.45 AM</td>\n",
       "      <td>03-APR-22 01.47.05 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CTLWT07ZY04671</td>\n",
       "      <td>WEB</td>\n",
       "      <td>PURCHASEEQUIPMENT</td>\n",
       "      <td>Completed</td>\n",
       "      <td>03-APR-22 02.53.21 AM</td>\n",
       "      <td>03-APR-22 02.53.45 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CTLWT087W05034</td>\n",
       "      <td>WEB</td>\n",
       "      <td>CHANGEMSISDN</td>\n",
       "      <td>Completed</td>\n",
       "      <td>03-APR-22 02.57.32 AM</td>\n",
       "      <td>03-APR-22 02.57.49 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300006</th>\n",
       "      <td>CTLSD00GC90980</td>\n",
       "      <td>WEB</td>\n",
       "      <td>PURCHASEEQUIPMENT</td>\n",
       "      <td>Completed</td>\n",
       "      <td>10-SEP-22 12.10.11 AM</td>\n",
       "      <td>10-SEP-22 12.10.16 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300007</th>\n",
       "      <td>CTLSD00W993338</td>\n",
       "      <td>WEB</td>\n",
       "      <td>CHANGEMSISDN</td>\n",
       "      <td>Completed</td>\n",
       "      <td>10-SEP-22 12.19.21 AM</td>\n",
       "      <td>10-SEP-22 12.23.49 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300008</th>\n",
       "      <td>CTLSD036W16397</td>\n",
       "      <td>WEB</td>\n",
       "      <td>CHANGEMSISDN</td>\n",
       "      <td>Completed</td>\n",
       "      <td>10-SEP-22 01.08.56 AM</td>\n",
       "      <td>10-SEP-22 01.09.29 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300009</th>\n",
       "      <td>CTLSD09PF44071</td>\n",
       "      <td>WEB</td>\n",
       "      <td>CHANGEMSISDN</td>\n",
       "      <td>Completed</td>\n",
       "      <td>10-SEP-22 03.29.39 AM</td>\n",
       "      <td>10-SEP-22 03.29.52 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300010</th>\n",
       "      <td>CTLSD0G2176054</td>\n",
       "      <td>IVR</td>\n",
       "      <td>PURCHASEEQUIPMENT</td>\n",
       "      <td>Completed</td>\n",
       "      <td>10-SEP-22 05.47.03 AM</td>\n",
       "      <td>10-SEP-22 05.47.10 AM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300011 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ORDERID CHANNELID          ORDERTYPE ORDERSTATUS  \\\n",
       "0       CTLWT07HI04104       WEB  PURCHASEEQUIPMENT   Completed   \n",
       "1       CTLWT07ND04290       WEB       CHANGEMSISDN   Completed   \n",
       "2       CTLWT04XX99611       WEB       CHANGEMSISDN   Completed   \n",
       "3       CTLWT07ZY04671       WEB  PURCHASEEQUIPMENT   Completed   \n",
       "4       CTLWT087W05034       WEB       CHANGEMSISDN   Completed   \n",
       "...                ...       ...                ...         ...   \n",
       "300006  CTLSD00GC90980       WEB  PURCHASEEQUIPMENT   Completed   \n",
       "300007  CTLSD00W993338       WEB       CHANGEMSISDN   Completed   \n",
       "300008  CTLSD036W16397       WEB       CHANGEMSISDN   Completed   \n",
       "300009  CTLSD09PF44071       WEB       CHANGEMSISDN   Completed   \n",
       "300010  CTLSD0G2176054       IVR  PURCHASEEQUIPMENT   Completed   \n",
       "\n",
       "                 CREATIONDATE        LASTUPDATEDDATE  \n",
       "0       03-APR-22 02.44.06 AM  03-APR-22 02.44.28 AM  \n",
       "1       03-APR-22 02.45.13 AM  03-APR-22 02.45.38 AM  \n",
       "2       03-APR-22 01.46.45 AM  03-APR-22 01.47.05 AM  \n",
       "3       03-APR-22 02.53.21 AM  03-APR-22 02.53.45 AM  \n",
       "4       03-APR-22 02.57.32 AM  03-APR-22 02.57.49 AM  \n",
       "...                       ...                    ...  \n",
       "300006  10-SEP-22 12.10.11 AM  10-SEP-22 12.10.16 AM  \n",
       "300007  10-SEP-22 12.19.21 AM  10-SEP-22 12.23.49 AM  \n",
       "300008  10-SEP-22 01.08.56 AM  10-SEP-22 01.09.29 AM  \n",
       "300009  10-SEP-22 03.29.39 AM  10-SEP-22 03.29.52 AM  \n",
       "300010  10-SEP-22 05.47.03 AM  10-SEP-22 05.47.10 AM  \n",
       "\n",
       "[300011 rows x 6 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2 as ps\n",
    "import pymysql\n",
    "driver = 'mysql+pymysql'\n",
    "server = '127.0.0.1'\n",
    "port = '3306'\n",
    "database = 'sales'\n",
    "username = 'root'\n",
    "password = 'K85southcity1'\n",
    "\n",
    "connection_string = f\"{driver}://{username}:{password}@{server}:{port}/{database}\"\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(connection_string)\n",
    "query = \"SELECT * FROM combined\"\n",
    "df = pd.read_sql(query, engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72e4e88b",
   "metadata": {},
   "source": [
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd67a703",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "query = \"SELECT * FROM combined\"\n",
    "df = pd.read_sql(query, engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8655f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Orders Dataset shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c34c7",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b9cd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect the Orders dataset\n",
    "print(df.index.is_unique)\n",
    "print(df.columns.is_unique)\n",
    "duplicate = df[df.duplicated()]\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a80017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicate values\n",
    "df.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = df[df.duplicated()]\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a979941f",
   "metadata": {},
   "source": [
    "The orders data set is ready to use as no duplicated values for rows and columns, all of them are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a69031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "print(df['CHANNELID'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8fabc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a81387",
   "metadata": {},
   "source": [
    "As noted as above, there's no missing values for the Orders dataset, we can now move on to the inspection on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800856ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def converter(x):\n",
    "    try:\n",
    "        return(datetime.strptime(x, \"%d-%b-%y %I.%M.%S %p\").strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "    except:\n",
    "        return x\n",
    "df['CREATIONDATE'] = df['CREATIONDATE'].apply(converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cd60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['ORDERID','LASTUPDATEDDATE'])\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc16e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee981f7",
   "metadata": {},
   "source": [
    "With the successful transformation of the dataset, we begin our exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14996f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have a DataFrame 'df' and 'top' defined\n",
    "\n",
    "# Building a figure\n",
    "fig = plt.figure(constrained_layout=True, figsize=(25, 9))\n",
    "gs = GridSpec(1, 3, figure=fig)\n",
    "ax1 = fig.add_subplot(gs[0, 1:3])\n",
    "\n",
    "# Countplot for ORDERTYPE\n",
    "sns.set_palette(\"pastel\")  # Set a pastel color palette\n",
    "top=len(df['ORDERTYPE'].value_counts().unique())\n",
    "\n",
    "\n",
    "order_counts = df['ORDERTYPE'].value_counts()\n",
    "top_orders = order_counts[:top]\n",
    "\n",
    "ax1 = sns.countplot(data=df, x='ORDERTYPE', order=top_orders.index, ax=ax1)\n",
    "\n",
    "ax1.set_title(f'Top order type with the Most Orders Received', color='dimgrey', size=18)\n",
    "\n",
    "for tick in ax1.get_xticklabels():\n",
    "    tick.set_rotation(25)\n",
    "\n",
    "# Annotate each bar with the count and percentage\n",
    "total_count = len(df['ORDERTYPE'])\n",
    "for p in ax1.patches:\n",
    "    height = p.get_height()\n",
    "    ax1.annotate(f'{height}\\n({height/total_count*100:.1f}%)',\n",
    "                 (p.get_x() + p.get_width() / 2., height),\n",
    "                 ha='center', va='center', fontsize=14, color='black', xytext=(0, 11),\n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17bd80c",
   "metadata": {},
   "source": [
    "From the graph above, we can see that PURCHASEEQUIPMENT is the most received order type, which represents a total of 41.2% of the whole group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66bd1f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Building a figure\n",
    "fig = plt.figure(constrained_layout=True, figsize=(25, 9))\n",
    "\n",
    "# Axis definition with GridSpec\n",
    "gs = GridSpec(1, 3, figure=fig)\n",
    "ax3 = fig.add_subplot(gs[0, 1:3])\n",
    "\n",
    "# Axis 3 - Top 20 Customers with the Most Orders\n",
    "top=len(df['CHANNELID'].value_counts().unique())\n",
    "\n",
    "sns.countplot(data=df, x='CHANNELID', order=df['CHANNELID'].value_counts().index[:top])\n",
    "\n",
    "ax3.set_title(f'Channel with the Most Orders', color='dimgrey', size=18)\n",
    "\n",
    "for tick in ax3.get_xticklabels():\n",
    "    tick.set_rotation(25)\n",
    "for p in ax3.patches:\n",
    "    height = p.get_height()\n",
    "    ax3.annotate(f'{height}\\n({height/total_count*100:.1f}%)',\n",
    "                 (p.get_x() + p.get_width() / 2., height),\n",
    "                 ha='center', va='center', fontsize=14, color='black', xytext=(0, 11),\n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549bff23",
   "metadata": {},
   "source": [
    "From the graph above, we can see that orders are mostly received through WEB, which represents a total of 38.4% of the whole group. Also we have a CHANNELID with no name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72059fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the data type for date columns\n",
    "timestamp_cols = ['CREATIONDATE']\n",
    "for col in timestamp_cols:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "# Extracting attributes for Order Date - Hour and Time of the Day\n",
    "df['ORDERDATEHOUR'] = df['CREATIONDATE'].apply(lambda x: x.hour)\n",
    "hours_bins = [0, 5, 11, 16, 20, 23]\n",
    "hours_labels = ['Midnight', 'Morning', 'Afternoon', 'Evening', 'Night']\n",
    "df['ORDERDATEHOURDAY'] = pd.cut(df['ORDERDATEHOUR'], hours_bins, labels=hours_labels,include_lowest=True)\n",
    "\n",
    "# New DataFrame after transformations\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c003e7",
   "metadata": {},
   "source": [
    "We are adding two new columns which represents the time interval during which an order was placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da41a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Building a figure\n",
    "fig = plt.figure(constrained_layout=True, figsize=(25, 9))\n",
    "\n",
    "# Axis definition with GridSpec\n",
    "gs = GridSpec(1, 3, figure=fig)\n",
    "ax4 = fig.add_subplot(gs[0, 0])\n",
    "ax5 = fig.add_subplot(gs[0, 1:2])\n",
    "\n",
    "# Axis 4 - Total Orders by Time Period of the Day\n",
    "colors = ['darkslateblue', 'cornflowerblue', 'silver', 'darkviolet', 'orange']\n",
    "label_names = df['ORDERDATEHOURDAY'].value_counts().index\n",
    "\n",
    "# Calculate the total counts for each category \n",
    "data = df['ORDERDATEHOURDAY'].value_counts()\n",
    "data = data.reindex(label_names, fill_value=0)  # Ensure all categories are present\n",
    "ax4.pie(data, labels=label_names, colors=colors, autopct='%1.1f%%', startangle=90, wedgeprops=dict(width=0.3))\n",
    "ax4.add_artist(plt.Circle((0, 0), 0.6, color='white'))  # Add a white circle for the \"donut\" effect\n",
    "ax4.set_title(f'Total Orders by Time Period of the Day', color='dimgrey', size=18)\n",
    "\n",
    "# Barchart Axis 5 - Total Orders by Order Hour of the day\n",
    "sns.countplot(df, x='ORDERDATEHOUR', ax=ax5, order=range(24))\n",
    "ax5.set_title('Total Orders by Order Hour of the Day', size=18, color='dimgrey', pad=20)\n",
    "for p in ax5.patches:\n",
    "    ax5.annotate(format(p.get_height(), '.0f'),(p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', \n",
    "                    va = 'center',fontsize=7, color='black', xytext = (0, 5),\n",
    "                 textcoords = 'offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc597633",
   "metadata": {},
   "source": [
    "From the graphs above, we can see that customers often order during Morning and Afternoon period mainly between 8AM and 1PM in a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47a76b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for the cross-tabulation of ORDERTYPE and CHANNELID\n",
    "cross_tab = pd.crosstab(df['ORDERTYPE'], df['CHANNELID'])\n",
    "\n",
    "# Build a figure for the heatmap\n",
    "fig_heatmap = plt.figure(constrained_layout=True, figsize=(14, 10))\n",
    "\n",
    "# Define a new GridSpec for the heatmap\n",
    "gs_heatmap = GridSpec(1, 1, figure=fig_heatmap)\n",
    "ax_heatmap = fig_heatmap.add_subplot(gs_heatmap[0, 0])\n",
    "\n",
    "# Create a heatmap using Seaborn\n",
    "sns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlGnBu', ax=ax_heatmap)\n",
    "\n",
    "ax_heatmap.set_title('Heatmap: ORDERTYPE vs. CHANNELID', color='dimgrey', size=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6613dc",
   "metadata": {},
   "source": [
    "This heat map mainly tells us which ORDERTYPE is placed through which CHANNELID. Notable finding is that CARE has more number of ORDERTYPE than WEB with some specific ORDERTYPE being received only through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a54a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have ax3 and ax5 defined as in your code\n",
    "import plotly.express as px\n",
    "\n",
    "# Create a new figure for the relationship\n",
    "fig_relation = plt.figure(constrained_layout=True, figsize=(15, 9))\n",
    "\n",
    "# Axis definition with GridSpec\n",
    "gs_relation = GridSpec(1, 1, figure=fig_relation)\n",
    "ax_relation = fig_relation.add_subplot(gs_relation[0, 0])\n",
    "\n",
    "# Create a relationship between \"Channel with the Most Orders\" and \"Total Orders by Order Hour of the Day\"\n",
    "# Loop through the unique channels and create a bar chart for each channel\n",
    "channels = df['CHANNELID'].unique()\n",
    "colors = sns.color_palette(\"husl\", len(channels))\n",
    "\n",
    "for i, channel in enumerate(channels):\n",
    "    channel_data = df[df['CHANNELID'] == channel]\n",
    "    sns.countplot(data=channel_data, x='ORDERDATEHOUR', ax=ax_relation, order=range(24), color=colors[i], alpha=0.7,\n",
    "                  label=f'{channel}')\n",
    "    \n",
    "# Set labels and titles\n",
    "ax_relation.set_xlabel('Order Hour of the Day')\n",
    "ax_relation.set_ylabel('Count of Orders')\n",
    "ax_relation.set_title('Relationship between Channel and Order Hour', size=18, color='dimgrey')\n",
    "\n",
    "# Add a legend\n",
    "ax_relation.legend(title='Channels', loc='upper right')\n",
    "for p in ax_relation.patches:\n",
    "    ax_relation.annotate(format(p.get_height(), '.0f'),(p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', \n",
    "                    va = 'center',fontsize=12, color='black', xytext = (0, 5),\n",
    "                 textcoords = 'offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463230e8",
   "metadata": {},
   "source": [
    "This graph tells that RETAIL orders are placed more than WEB orders between 10AM and 3PM. WEB orders lead in the remaining time throughout the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ed0d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the counts for each combination of channel and order hour\n",
    "heatmap_data = df.groupby(['CHANNELID', 'ORDERDATEHOUR']).size().unstack(fill_value=0)\n",
    "\n",
    "# Create a figure and axis for the heatmap\n",
    "plt.figure(figsize=(16, 8))\n",
    "ax_heatmap = sns.heatmap(heatmap_data, cmap='YlGnBu', annot=True, fmt='d', linewidths=.5)\n",
    "\n",
    "# Set labels and title\n",
    "ax_heatmap.set_xlabel('Order Hour of the Day')\n",
    "ax_heatmap.set_ylabel('Channel')\n",
    "ax_heatmap.set_title('Heatmap: Channel vs. Order Hour', size=18, color='dimgrey')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f8948",
   "metadata": {},
   "source": [
    "This heat map gives a better visualisation of the above graph. It tells that CARE and IVR orders start receiving earlier that RETAIL orders in the morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc77206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have ax3 and ax5 defined as in your code\n",
    "\n",
    "# Create a new figure for the relationship\n",
    "fig_relation = plt.figure(constrained_layout=True, figsize=(15, 9))\n",
    "\n",
    "# Axis definition with GridSpec\n",
    "gs_relation = GridSpec(1, 1, figure=fig_relation)\n",
    "ax_relation = fig_relation.add_subplot(gs_relation[0, 0])\n",
    "\n",
    "# Create a relationship between \"Channel with the Most Orders\" and \"Total Orders by Order Hour of the Day\"\n",
    "# Loop through the unique channels and create a bar chart for each channel\n",
    "types = df['ORDERTYPE'].unique()\n",
    "colors = sns.color_palette(\"husl\", len(types))\n",
    "\n",
    "for i, type in enumerate(types):\n",
    "    type_data = df[df['ORDERTYPE'] == type]\n",
    "    sns.countplot(data=type_data, x='ORDERDATEHOUR', ax=ax_relation, order=range(24), color=colors[i], alpha=0.7,\n",
    "                  label=f'{type}')\n",
    "    \n",
    "# Set labels and titles\n",
    "ax_relation.set_xlabel('Order Hour of the Day')\n",
    "ax_relation.set_ylabel('Count of Orders')\n",
    "ax_relation.set_title('Relationship between Order Type and Order Hour', size=18, color='dimgrey')\n",
    "ax_relation.legend(title='Channels', loc='upper right')\n",
    "ax_relation.legend(title='Channels', loc='upper right')\n",
    "for p in ax_relation.patches:\n",
    "    ax_relation.annotate(format(p.get_height(), '.0f'),(p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', \n",
    "                    va = 'center',fontsize=12, color='black', xytext = (0, 5),\n",
    "                 textcoords = 'offset points')\n",
    "# Add a legend\n",
    "ax_relation.legend(title='Types', loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2c4b3",
   "metadata": {},
   "source": [
    "This graph tells that PURCHASEEQUIPMENT are recevind mostly during morning, afternoon and evening while CHANGEMSISDN is received almost equally throughout all time intervals. UPDATECUSTOMER starts in morning and lasts till evening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff84b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the counts for each combination of order type and order hour\n",
    "heatmap_data = df.groupby(['ORDERTYPE', 'ORDERDATEHOUR']).size().unstack(fill_value=0)\n",
    "\n",
    "# Create a figure and axis for the heatmap\n",
    "plt.figure(figsize=(16, 8))\n",
    "ax_heatmap = sns.heatmap(heatmap_data, cmap='YlGnBu', annot=True, fmt='d', linewidths=.5)\n",
    "\n",
    "# Set labels and title\n",
    "ax_heatmap.set_xlabel('Order Hour of the Day')\n",
    "ax_heatmap.set_ylabel('Order Type')\n",
    "ax_heatmap.set_title('Heatmap: Order Type vs. Order Hour', size=18, color='dimgrey')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['CREATIONDATE'].dt.date\n",
    "df['Hour'] = df['CREATIONDATE'].dt.hour\n",
    "hourly_counts = df.groupby(['Date', 'Hour','ORDERTYPE']).size().reset_index(name='Count')\n",
    "hourly_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hourly_counts['Count'] = np.log(hourly_counts['Count'] + 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74899103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate theoretical quantiles for a normal distribution\n",
    "theoretical_quantiles = np.linspace(0, 1, len(hourly_counts))\n",
    "theoretical_values = stats.norm.ppf(theoretical_quantiles)\n",
    "\n",
    "# Sort the observed values\n",
    "sorted_observed_values = np.sort(hourly_counts['Count'])\n",
    "\n",
    "# Plot the QQ plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(theoretical_values, sorted_observed_values)\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Ordered Values')\n",
    "plt.title('QQ Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot that exhibit this behavior usually mean that our  data has more extreme values than would be expected if they truly came from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad728143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Plot the ACF\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_acf(hourly_counts['Count'], lags=24) \n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')\n",
    "plt.title('Autocorrelation Function (ACF) Plot')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# Create a PACF plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plot_pacf(hourly_counts['Count'], lags=50)  \n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Partial Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca87ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e009b",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23cd824",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad210dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702da7a",
   "metadata": {},
   "source": [
    "# ARIMA INPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8433ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the orders count data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(hourly_counts['Count'])\n",
    "plt.title('Hourly Orders Count')\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Orders Count')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arima\n",
    "hourly_counts['Count_diff'] = hourly_counts['Count'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(hourly_counts) * 0.8)\n",
    "train_data = hourly_counts.iloc[:train_size]\n",
    "test_data = hourly_counts.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p, d, q = 1, 1, 1  # Example values; adjust based on your data\n",
    "\n",
    "# Create and fit the ARIMA model\n",
    "model = sm.tsa.arima.ARIMA(train_data['Count_diff'], order=(p, d, q))\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = results.get_forecast(steps=len(test_data))\n",
    "forecast_mean = forecast.predicted_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data.index, train_data['Count_diff'], label='Train Data')\n",
    "plt.plot(test_data.index, test_data['Count_diff'], label='Test Data')\n",
    "plt.plot(test_data.index, forecast_mean, label='ARIMA Forecast')\n",
    "plt.title('ARIMA Forecast vs. Actual')\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Orders Count Difference')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55b3d6",
   "metadata": {},
   "source": [
    "# SARIMA IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc1465b5",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Assuming you have a DataFrame 'hourly_counts' with 'Hour', 'OrderType', and 'Count' columns\n",
    "window_size = 12\n",
    "\n",
    "# Create a function to fit SARIMA and forecast\n",
    "def fit_sarima_forecast(data):\n",
    "    model = SARIMAX(data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))  # Modify order and seasonal_order as needed\n",
    "    results = model.fit()\n",
    "    forecast = results.get_forecast(steps=len(data))\n",
    "    return forecast\n",
    "\n",
    "# Group the data by hour and order type, and forecast with SARIMA for each group\n",
    "hourly_counts['Forecast'] = hourly_counts.groupby(['Hour', 'ORDERTYPE'])['Count'].transform(fit_sarima_forecast)\n",
    "\n",
    "# Calculate percentage change\n",
    "hourly_counts['Percentage_change'] = ((hourly_counts['Count'] - hourly_counts['Forecast']) / hourly_counts['Forecast']).abs()\n",
    "\n",
    "# Set the threshold for alerts\n",
    "threshold = 0.1  # Modify as needed\n",
    "\n",
    "# Check if the percentage change exceeds the threshold for each combination\n",
    "hourly_counts['Alert'] = hourly_counts['Percentage_change'] > threshold\n",
    "\n",
    "# Plot the alerts\n",
    "plt.figure(figsize=(15, 4))\n",
    "for order_type, data in hourly_counts.groupby('ORDERTYPE'):\n",
    "    plt.plot(data.index, data['Count'], label=f'Order Type {order_type}')\n",
    "    plt.plot(data.index, data['Forecast'], label=f'SARIMA Forecast Order Type {order_type}')\n",
    "    alerts = data[data['Alert']]\n",
    "    plt.scatter(alerts.index, alerts['Count'], color='red', marker='o', label=f'Alert Order Type {order_type}')\n",
    "    plt.xlabel('Hourly Timestamp')\n",
    "    plt.ylabel('Hourly Orders')\n",
    "    plt.title('Hourly Order Trend Analysis by Order Type (SARIMA)')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the order change for each alert\n",
    "hourly_counts['OrderChange'] = hourly_counts['Count'] - hourly_counts['Forecast']\n",
    "\n",
    "# Select rows where alerts are triggered\n",
    "alerts = hourly_counts[hourly_counts['Alert']]\n",
    "\n",
    "# Create a dictionary to store the number of order changes for each alert\n",
    "alert_changes = {}\n",
    "\n",
    "# Iterate through the alerts and calculate the number of order changes for each one\n",
    "for index, row in alerts.iterrows():\n",
    "    hour, order_type = row['Hour'], row['ORDERTYPE']\n",
    "    order_change = row['OrderChange']\n",
    "    alert_changes[(hour, order_type)] = order_change\n",
    "\n",
    "# Print the number of order changes for each alert\n",
    "for (hour, order_type), order_change in alert_changes.items():\n",
    "    print(f\"Alert Time: {hour}, Order Type: {order_type}, Order Change: {order_change}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a191eaa0",
   "metadata": {},
   "source": [
    "#sarima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Define SARIMA model parameters\n",
    "p, d, q = 1, 1, 1\n",
    "P, D, Q, seasonal_period = 1, 1, 1, 24  \n",
    "\n",
    "# Create and fit the SARIMA model\n",
    "model = SARIMAX(train_data['Count_diff'], order=(p, d, q), seasonal_order=(P, D, Q, seasonal_period))\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfdfab45",
   "metadata": {},
   "source": [
    "forecast = results.get_forecast(steps=len(test_data))\n",
    "forecast_mean = forecast.predicted_mean"
   ]
  },
  {
   "cell_type": "raw",
   "id": "984b7761",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data.index, train_data['Count_diff'], label='Train Data')\n",
    "plt.plot(test_data.index, test_data['Count_diff'], label='Test Data')\n",
    "plt.plot(test_data.index, forecast_mean, label='SARIMA Forecast')\n",
    "plt.title('SARIMA Forecast vs. Actual')\n",
    "plt.xlabel('Date and Time')\n",
    "plt.ylabel('Orders Count Difference')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5792924",
   "metadata": {},
   "source": [
    "# Perform differencing to make the data stationary \n",
    "hourly_counts['Order_count_diff'] = hourly_counts['Count'].diff().fillna(0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42ecc4f0",
   "metadata": {},
   "source": [
    "hourly_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b584e",
   "metadata": {},
   "source": [
    "# REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5981617",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(hourly_counts))\n",
    "train_data = hourly_counts.iloc[:train_size]\n",
    "test_data = hourly_counts.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_diff = results.get_forecast(steps=len(hourly_counts))\n",
    "forecast_mean_diff = forecast_diff.predicted_mean\n",
    "forecast_mean = hourly_counts['Count'].iloc[train_size - 1] + forecast_mean_diff.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(hourly_counts['Count'], forecast_mean)\n",
    "rmse = np.sqrt(mean_squared_error(hourly_counts['Count'], forecast_mean))\n",
    "mape = np.mean(np.abs((hourly_counts['Count'] - forecast_mean) / hourly_counts['Count'])) * 100\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c7c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 15.0  # Adjust this threshold based on your data and requirements\n",
    "\n",
    "if mape > threshold:\n",
    "    alert_message = f\"Deviation detected: MAPE = {mape:.2f}% (Threshold = {threshold:.2f}%)\"\n",
    "    print(alert_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5c6f3",
   "metadata": {},
   "source": [
    "# SMA IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed224c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a DataFrame 'hourly_counts' with 'Hour', 'OrderType', and 'Count' columns\n",
    "window_size = 24\n",
    "hourly_counts['sma'] = hourly_counts.groupby(['Hour', 'ORDERTYPE'])['Count'].transform(lambda x: x.rolling(window=window_size).mean())\n",
    "hourly_counts['Percentage_change'] = ((hourly_counts['Count'] - hourly_counts['sma']) / hourly_counts['sma']).abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5329a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation of percentage changes for each unique combination\n",
    "grouped = hourly_counts.groupby(['Hour', 'ORDERTYPE'])['Percentage_change']\n",
    "mean_percentage_change = grouped.transform('mean')\n",
    "std_dev_percentage_change = grouped.transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29931bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for each combination \n",
    "threshold = 5 * std_dev_percentage_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the percentage change exceeds the threshold for each combination\n",
    "hourly_counts['Alert'] = hourly_counts['Percentage_change'] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30881dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the alerts\n",
    "plt.figure(figsize=(15, 4))\n",
    "for order_type, data in hourly_counts.groupby('ORDERTYPE'):\n",
    "    plt.plot(data.index, data['Count'], label=f'Order Type {order_type}')\n",
    "    plt.plot(data.index, data['sma'], label=f'SMA ({window_size}-hour) Order Type {order_type}')\n",
    "    alerts = data[data['Alert']]\n",
    "    plt.scatter(alerts.index, alerts['Count'], color='red', marker='o', label=f'Alert Order Type {order_type}')\n",
    "    plt.xlabel('Hourly Timestamp')\n",
    "    plt.ylabel('Hourly Orders')\n",
    "    plt.title('Hourly Order Trend Analysis by Order Type')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd681d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the order change for each alert\n",
    "hourly_counts['OrderChange'] = hourly_counts['Count'] - hourly_counts['sma']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where alerts are triggered\n",
    "alerts = hourly_counts[hourly_counts['Alert']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the number of order changes for each alert\n",
    "alert_changes = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f31b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the alerts and calculate the number of order changes for each one\n",
    "for index, row in alerts.iterrows():\n",
    "    alert_time = index\n",
    "    order_change = row['OrderChange']\n",
    "    alert_changes[(row['Hour'], row['ORDERTYPE'])] = order_change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dea523",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the number of order changes for each alert\n",
    "for (hour, order_type), order_change in alert_changes.items():\n",
    "    print(f\"Alert Time: {hour}, Order Type: {order_type}, Order Change: {order_change}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88ac8faa",
   "metadata": {},
   "source": [
    "# Calculate a simple moving average (SMA) for hourly orders\n",
    "window_size =12\n",
    "hourly_counts['sma'] = hourly_counts['Count'].rolling(window=window_size).mean()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a975ef4",
   "metadata": {},
   "source": [
    "# Calculate the percentage change from the moving average\n",
    "hourly_counts['Percentage_change'] = ((hourly_counts['Count'] - hourly_counts['sma']) / hourly_counts['sma']).abs()\n",
    "\n",
    "# Assuming you have a pandas DataFrame 'hourly_counts' with a 'Percentage_change' column\n",
    "percentage_changes = hourly_counts['Percentage_change']\n",
    "\n",
    "# Calculate the mean and standard deviation of percentage changes\n",
    "mean_percentage_change = percentage_changes.mean()\n",
    "std_dev_percentage_change = percentage_changes.std()\n",
    "\n",
    "# Set the threshold (e.g., 2 times the standard deviation)\n",
    "threshold = 4 * std_dev_percentage_change\n",
    "\n",
    "# You can print or use the threshold as needed\n",
    "print(f\"Threshold: {threshold:.2f}\")\n",
    "\n",
    "# Check if the percentage change exceeds the threshold\n",
    "\n",
    "hourly_counts['Alert'] = hourly_counts['Percentage_change'] > threshold"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96d433a8",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(hourly_counts.index, hourly_counts['Count'], label='Hourly Orders')\n",
    "plt.plot(hourly_counts.index, hourly_counts['sma'], label=f'SMA ({window_size}-hour)')\n",
    "plt.scatter(hourly_counts.index[hourly_counts['Alert']], hourly_counts['Count'][hourly_counts['Alert']], color='red', marker='o', label='Alert')\n",
    "\n",
    "plt.xlabel('Hourly Timestamp')\n",
    "plt.ylabel('Hourly Orders')\n",
    "plt.title('Hourly Order Trend Analysis')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "944a165a",
   "metadata": {},
   "source": [
    "# Calculate the order change for each alert\n",
    "hourly_counts['order_change'] = hourly_counts['Count'] - hourly_counts['sma']\n",
    "\n",
    "# Select rows where alerts are triggered\n",
    "alerts = hourly_counts[hourly_counts['Alert']]\n",
    "\n",
    "# Create a dictionary to store the number of order changes for each alert\n",
    "alert_changes = {}\n",
    "\n",
    "# Iterate through the alerts and calculate the number of order changes for each one\n",
    "for alert_time in alerts.index:\n",
    "    alert_change = alerts.loc[alert_time, 'order_change']\n",
    "    alert_changes[alert_time] = alert_change\n",
    "\n",
    "# Print the number of order changes for each alert\n",
    "for alert_time, order_change in alert_changes.items():\n",
    "    print(f\"Alert Time: {alert_time}, Order Change: {order_change }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ab50e8",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "window_size = 24\n",
    "hourly_counts['sma'] = hourly_counts.groupby(['Hour', 'ORDERTYPE'])['Count'].transform(lambda x: x.rolling(window=window_size).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Z-Score for each data point\n",
    "hourly_counts['Z-Score'] = (hourly_counts['Count'] - hourly_counts['sma']) / hourly_counts.groupby(['Hour', 'ORDERTYPE'])['Count'].transform('std')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ee102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for anomaly detection \n",
    "threshold = 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a33a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify anomalies based on the threshold\n",
    "hourly_counts['Anomaly'] = np.abs(hourly_counts['Z-Score']) > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed41b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the anomalies\n",
    "plt.figure(figsize=(16, 5))\n",
    "for order_type, data in hourly_counts.groupby('ORDERTYPE'):\n",
    "    plt.plot(data.index, data['Count'], label=f'Order Type {order_type}')\n",
    "    plt.plot(data.index, data['sma'], label=f'SMA ({window_size}-hour) Order Type {order_type}')\n",
    "    anomalies = data[data['Anomaly']]\n",
    "    plt.scatter(anomalies.index, anomalies['Count'], color='red', marker='o', label=f'Anomaly Order Type {order_type}')\n",
    "    plt.xlabel('Hourly Timestamp')\n",
    "    plt.ylabel('Hourly Orders')\n",
    "    plt.title('Hourly Order Anomaly Detection by Order Type')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b3340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify and print the anomalies\n",
    "anomalies = hourly_counts[hourly_counts['Anomaly']]\n",
    "for index, row in anomalies.iterrows():\n",
    "    hour, order_type = row['Hour'], row['ORDERTYPE']\n",
    "    anomaly_value = row['Count']\n",
    "    print(f\"Anomaly Detected - Hour: {hour}, Order Type: {order_type}, Value: {anomaly_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394f8d5",
   "metadata": {},
   "source": [
    "# using Prophet "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccd3fd23",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "\n",
    "# Assuming you have a DataFrame 'hourly_counts' with 'Hour' and 'Count' columns\n",
    "data = hourly_counts.rename(columns={'Hour': 'ds', 'Count': 'y'})\n",
    "\n",
    "# Initialize Prophet model\n",
    "model = Prophet()\n",
    "model.fit(data)\n",
    "\n",
    "# Make forecasts\n",
    "forecast = model.predict(data)\n",
    "\n",
    "# Calculate residuals (observed - predicted)\n",
    "data['residuals'] = data['y'] - forecast['yhat']\n",
    "\n",
    "# Calculate upper and lower bounds for anomalies based on prediction intervals\n",
    "data['upper_bound'] = forecast['yhat_upper']\n",
    "data['lower_bound'] = forecast['yhat_lower']\n",
    "\n",
    "# Define a threshold for anomaly detection (e.g., 2 times the prediction interval width)\n",
    "threshold = 2 * (data['upper_bound'] - data['lower_bound'])\n",
    "\n",
    "# Detect anomalies\n",
    "data['is_anomaly'] = (data['residuals'].abs() > threshold).astype(int)\n",
    "\n",
    "# Plot the anomalies\n",
    "# You can customize the plot as needed\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(data['ds'], data['y'], label='Observed')\n",
    "plt.plot(data['ds'], forecast['yhat'], label='Predicted')\n",
    "plt.fill_between(data['ds'], data['upper_bound'], data['lower_bound'], alpha=0.2, color='gray', label='Prediction Interval')\n",
    "anomalies = data[data['is_anomaly'] == 1]\n",
    "plt.scatter(anomalies['ds'], anomalies['y'], color='red', marker='o', label='Anomalies')\n",
    "plt.xlabel('Hourly Timestamp')\n",
    "plt.ylabel('Hourly Orders')\n",
    "plt.title('Hourly Order Anomaly Detection using Prophet')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the detected anomalies\n",
    "detected_anomalies = data[data['is_anomaly'] == 1][['ds', 'y']]\n",
    "print(\"Detected Anomalies:\")\n",
    "print(detected_anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3157091",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19bd0b8f",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Assuming you have a DataFrame 'hourly_counts' with 'Hour' and 'Count' columns\n",
    "window_size = 24  # Length of input sequences\n",
    "threshold = 2.0  # Anomaly detection threshold\n",
    "\n",
    "# Data Preprocessing\n",
    "scaler = MinMaxScaler()\n",
    "hourly_counts['Count'] = scaler.fit_transform(hourly_counts['Count'].values.reshape(-1, 1))\n",
    "\n",
    "# Create Sequences\n",
    "sequences = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(hourly_counts) - window_size):\n",
    "    sequences.append(hourly_counts['Count'][i:i+window_size])\n",
    "    targets.append(hourly_counts['Count'][i+window_size])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = np.array(targets)\n",
    "\n",
    "# Train-Test Split\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Build LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(window_size, 1), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Model Evaluation\n",
    "predictions = model.predict(X_test)\n",
    "residuals = y_test - predictions\n",
    "\n",
    "# Anomaly Detection\n",
    "anomalies = np.abs(residuals) > threshold\n",
    "\n",
    "# Plot Results\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.plot(hourly_counts.index[split_index+window_size:], hourly_counts['Count'][split_index+window_size:], label='Original Data')\n",
    "plt.plot(hourly_counts.index[split_index+window_size:], predictions, label='Predicted Values')\n",
    "plt.scatter(hourly_counts.index[split_index+window_size:][anomalies.flatten()], hourly_counts['Count'][split_index+window_size:][anomalies.flatten()], color='red', marker='o', label='Anomalies')\n",
    "plt.xlabel('Hourly Timestamp')\n",
    "plt.ylabel('Scaled Hourly Orders')\n",
    "plt.title('Hourly Order Anomaly Detection using LSTM')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Anomaly Detection\n",
    "anomalies = np.abs(residuals.flatten()) > threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca5633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b3368cd",
   "metadata": {},
   "source": [
    "# RAISING ALERTS BASED ON DEVIATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bfa0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts = hourly_counts[hourly_counts['Alert']]\n",
    "print(alerts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a10db",
   "metadata": {},
   "source": [
    "# CONNECTING TO A SMTP SERVER TO SEND EMAIL NOTIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a68bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "import logging\n",
    "\n",
    "def send_email(subject, message):\n",
    "    sender_email = \"damboledheeraj@gmail.com\"  \n",
    "    receiver_email = \"aadibansal176@gmail.com\"  \n",
    "    password = \"znbf pdav ioox edzo\" \n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = receiver_email\n",
    "    msg['Subject'] = 'Alerts Report'\n",
    "\n",
    "    column_data_str = alerts.to_string(index=False)\n",
    "\n",
    "    email_body = f\"{column_data_str}\\n\\n{message}\"\n",
    "    msg.attach(MIMEText(email_body, 'plain'))\n",
    "    \n",
    "    try:\n",
    "        server = smtplib.SMTP(\"smtp.gmail.com\", 587) \n",
    "        server.starttls()\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, msg.as_string())\n",
    "        server.quit()\n",
    "        logging.info(f\"Email notification sent to {receiver_email}: {subject}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to send email notification: {str(e)}\")\n",
    "\n",
    "if threshold >= 0.4:\n",
    "    logging.warning('Alert: Standard number of orders has changed.')\n",
    "    custom_message = \"Standard number of orders has changed. Check the trend analyzer for detail. https://shorturl.at/bnryP\"\n",
    "    send_email(custom_message,\"Order Alert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2a3e6",
   "metadata": {},
   "source": [
    "# RUNNING OUR DATA FETCHING AND EMAIL NOTIFICATION PROGRAMS AT REGULAR TIME INTERVALS TO WORK ON REAL TIME DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6bf690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "\n",
    "def my_task():\n",
    "    def send_email(subject, message):\n",
    "        sender_email = \"damboledheeraj@gmail.com\"  # Your email address\n",
    "        receiver_email = \"mittaldevansh48@gmail.com\"  # Recipient's email address\n",
    "        password = \"znbf pdav ioox edzo\" \n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = sender_email\n",
    "        msg['To'] = receiver_email\n",
    "        msg['Subject'] = 'Alerts Report'\n",
    "\n",
    "        column_data_str = alerts.to_string(index=False)\n",
    "\n",
    "        email_body = f\"{column_data_str}\\n\\n{message}\"\n",
    "        msg.attach(MIMEText(email_body, 'plain'))\n",
    "\n",
    "        try:\n",
    "            server = smtplib.SMTP(\"smtp.gmail.com\", 587) \n",
    "            server.starttls()\n",
    "            server.login(sender_email, password)\n",
    "            server.sendmail(sender_email, receiver_email, msg.as_string())\n",
    "            server.quit()\n",
    "            logging.info(f\"Email notification sent to {receiver_email}: {subject}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to send email notification: {str(e)}\")\n",
    "\n",
    "    if threshold >= 0.4:\n",
    "        logging.warning('Alert: Standard number of orders has changed.')\n",
    "        custom_message = \"Standard number of orders has changed. Check the trend analyzer for detail. https://shorturl.at/bnryP\"\n",
    "        send_email(custom_message,\"Order Alert\")\n",
    "        print(\"ANALYSER WILL CHECK AGAIN IN 10 MINUTES\")\n",
    "\n",
    "schedule.every(10).minutes.do(my_task)\n",
    "\n",
    "# Run the scheduler\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)  # Sleep for 1 second to avoid high CPU usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SYSTUM",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
